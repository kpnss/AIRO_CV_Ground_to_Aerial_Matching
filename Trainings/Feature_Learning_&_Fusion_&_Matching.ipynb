{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from google.colab import drive\n",
        "import random\n",
        "from torch.utils.data import DataLoader, dataset"
      ],
      "metadata": {
        "id": "C_Ttu_nXf_xf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv4Zvd8bgbKe",
        "outputId": "8293ae5b-0eb9-4c9a-86e7-c9e722b62493"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MatchingDataset(Dataset):\n",
        "    def __init__(self, ground_dir, generated_dir, seg_dir, candidate_dir, transform=None):\n",
        "        self.ground_dir = ground_dir\n",
        "        self.generated_dir = generated_dir\n",
        "        self.seg_dir = seg_dir\n",
        "        self.candidate_dir = candidate_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.ground_files = sorted([f for f in os.listdir(ground_dir) if f.endswith(\".jpg\")])\n",
        "        self.generated_files = sorted([f for f in os.listdir(generated_dir) if f.endswith(\".png\")])\n",
        "        self.seg_files = sorted([f for f in os.listdir(seg_dir) if f.endswith(\".png\")])\n",
        "        self.candidate_files = sorted([f for f in os.listdir(candidate_dir) if f.endswith(\".png\")])\n",
        "\n",
        "        assert len(self.generated_files) == len(self.seg_files), \\\n",
        "            \"Mismatch tra immagini generate e segmentate!\"\n",
        "        assert len(self.generated_files) <= len(self.ground_files), \\\n",
        "            \"PiÃ¹ immagini generate che ground!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.generated_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ground_filename = self.ground_files[idx]\n",
        "        ground_path = os.path.join(self.ground_dir, ground_filename)\n",
        "        ground_img = Image.open(ground_path).convert(\"RGB\")\n",
        "\n",
        "        gen_filename = self.generated_files[idx]\n",
        "        gen_path = os.path.join(self.generated_dir, gen_filename)\n",
        "        gen_img = Image.open(gen_path).convert(\"RGB\")\n",
        "\n",
        "        seg_filename = self.seg_files[idx]\n",
        "        seg_path = os.path.join(self.seg_dir, seg_filename)\n",
        "        seg_img = Image.open(seg_path).convert(\"RGB\")\n",
        "\n",
        "        image_id = os.path.splitext(ground_filename)[0]\n",
        "        candidate_filename = f\"input{image_id}.png\"\n",
        "        candidate_path = os.path.join(self.candidate_dir, candidate_filename)\n",
        "        candidate_img = Image.open(candidate_path).convert(\"RGB\")\n",
        "\n",
        "        negative_idx = idx\n",
        "        while negative_idx == idx:\n",
        "            negative_idx = random.randint(0, len(self.candidate_files) - 1)\n",
        "        negative_filename = self.candidate_files[negative_idx]\n",
        "        negative_path = os.path.join(self.candidate_dir, negative_filename)\n",
        "        negative_img = Image.open(negative_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            ground_img = self.transform(ground_img)\n",
        "            gen_img = self.transform(gen_img)\n",
        "            seg_img = self.transform(seg_img)\n",
        "            candidate_img = self.transform(candidate_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return {\n",
        "            \"ground\": ground_img,\n",
        "            \"generated\": gen_img,\n",
        "            \"seg\": seg_img,\n",
        "            \"candidate\": candidate_img,\n",
        "            \"candidate_neg\": negative_img\n",
        "        }"
      ],
      "metadata": {
        "id": "u5Zd21taf-DO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_rgb = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = MatchingDataset(\n",
        "    ground_dir=\"/content/drive/MyDrive/Dataset_Computer_Vision/Dataset_CVUSA/Dataset_CVUSA/streetview\",\n",
        "    generated_dir=\"/content/drive/MyDrive/generated_images\",\n",
        "    seg_dir=\"/content/drive/MyDrive/generated_seg\",\n",
        "    candidate_dir=\"/content/drive/MyDrive/Dataset_Computer_Vision/Dataset_CVUSA/Dataset_CVUSA/bingmap\",\n",
        "    transform=transform_rgb\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=10)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2qH_spSCgKUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BG_krYK_lLSU"
      },
      "outputs": [],
      "source": [
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(vgg.features.children()))\n",
        "        self.pool = vgg.avgpool\n",
        "        self.fc = nn.Sequential(*list(vgg.classifier.children())[:-1])  # fino a penultimo layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class FeatureFusionNet(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "class JointFeatureLearningNet(nn.Module):\n",
        "    def __init__(self, pretrained=True, embed_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vgg_G = VGGFeatureExtractor(pretrained)\n",
        "        self.vgg_A = VGGFeatureExtractor(pretrained)\n",
        "        self.vgg_S = VGGFeatureExtractor(pretrained)\n",
        "        self.vgg_C = self.vgg_A\n",
        "\n",
        "        self.ffn_GAS = FeatureFusionNet(input_dim=4096*3, embed_dim=embed_dim)\n",
        "        self.ffn_AC = FeatureFusionNet(input_dim=4096*2, embed_dim=embed_dim)\n",
        "\n",
        "    def forward(self, G, A, S, C):\n",
        "        fG = self.vgg_G(G)\n",
        "        fA = self.vgg_A(A)\n",
        "        fS = self.vgg_S(S)\n",
        "        fC = self.vgg_C(C)\n",
        "\n",
        "        embed_G = self.ffn_GAS(torch.cat([fG, fA, fS], dim=1))\n",
        "        embed_C = self.ffn_AC(torch.cat([fA, fC], dim=1))\n",
        "        return embed_G, embed_C\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = JointFeatureLearningNet(pretrained=True, embed_dim=256).to(device)\n",
        "\n",
        "vgg_params = []\n",
        "ffn_params = []\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'vgg' in name:\n",
        "        vgg_params.append(param)\n",
        "    else:\n",
        "        ffn_params.append(param)\n",
        "\n",
        "triplet_loss = nn.TripletMarginLoss(margin=0.2, p=2)"
      ],
      "metadata": {
        "id": "CIHa7LObfaJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d94275-3b8f-4168-853e-8d2cc1fe5767"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|ââââââââââ| 528M/528M [00:02<00:00, 229MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = JointFeatureLearningNet().to(device)\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/checkpoints_feature/all_models_epoch05.pt', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(\"Modello e ottimizzatore ripristinati con successo.\")\n",
        "\n",
        "model.train()\n",
        "num_epochs = 5\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': vgg_params, 'lr': 1e-4},\n",
        "    {'params': ffn_params, 'lr': 1e-3},\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR3mGaHP4mqM",
        "outputId": "7c6eb2f4-ca91-46ee-8b76-5ef25d853399"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello e ottimizzatore ripristinati con successo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch in loader:\n",
        "        G = batch[\"ground\"].to(device)\n",
        "        A = batch[\"generated\"].to(device)\n",
        "        S = batch[\"seg\"].to(device)\n",
        "        C_pos = batch[\"candidate\"].to(device)\n",
        "        C_neg = batch[\"candidate_neg\"].to(device)\n",
        "\n",
        "        embed_G, embed_pos = model(G, A, S, C_pos)\n",
        "        _, embed_neg = model(G, A, S, C_neg)\n",
        "\n",
        "        loss = triplet_loss(embed_G, embed_pos, embed_neg)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "jx7nQEvkfnFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b69a0d-9e2b-4693-8934-f00bc02e87c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.1175\n",
            "Epoch 2/5 - Loss: 0.0165\n",
            "Epoch 3/5 - Loss: 0.0737\n",
            "Epoch 4/5 - Loss: 0.1505\n",
            "Epoch 5/5 - Loss: 0.0508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"checkpoints_feature\", exist_ok=True)"
      ],
      "metadata": {
        "id": "8fcE9WtTi-j-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'epoch': epoch + 1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, 'checkpoints_feature/all_models_epoch05.pt')\n",
        "\n",
        "print(\"Checkpoint salvato correttamente!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6z2rJzBi7oS",
        "outputId": "2b2301f9-f131-4b0e-9514-bd0864d918f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint salvato correttamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r checkpoints_feature drive/MyDrive/"
      ],
      "metadata": {
        "id": "oZZDMfoKtirq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_matching_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        G = batch[\"ground\"].to(device)\n",
        "        A = batch[\"generated\"].to(device)\n",
        "        S = batch[\"seg\"].to(device)\n",
        "        C_pos = batch[\"candidate\"].to(device)\n",
        "        C_neg = batch[\"candidate_neg\"].to(device)\n",
        "\n",
        "        embed_G, embed_pos = model(G, A, S, C_pos)\n",
        "        _, embed_neg = model(G, A, S, C_neg)\n",
        "\n",
        "        sim_pos = F.cosine_similarity(embed_G, embed_pos, dim=1)\n",
        "        sim_neg = F.cosine_similarity(embed_G, embed_neg, dim=1)\n",
        "\n",
        "        correct += (sim_pos > sim_neg).sum().item()\n",
        "        total += G.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Accuracy (pos > neg): {acc:.4f}\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "X-XwpR1wjgjP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acc = evaluate_matching_accuracy(\n",
        "    model=model,\n",
        "    dataloader=loader,\n",
        "    device=device\n",
        "    #k=5\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "id": "AfoCBYC3jjX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4d8584-da31-45d4-ffae-6f35e58456f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (pos > neg): 0.5016\n",
            "Accuracy: 0.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JWGGYoxj2LbJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "class MatchingEvalDataset(Dataset):\n",
        "    def __init__(self, ground_dir, generated_dir, seg_dir, candidate_dir,\n",
        "                 transform=None, num_candidates=10):\n",
        "        self.ground_dir = ground_dir\n",
        "        self.generated_dir = generated_dir\n",
        "        self.seg_dir = seg_dir\n",
        "        self.candidate_dir = candidate_dir\n",
        "        self.transform = transform\n",
        "        self.num_candidates = num_candidates\n",
        "\n",
        "        self.ground_files = sorted([f for f in os.listdir(ground_dir) if f.endswith(\".jpg\")])\n",
        "        self.generated_files = sorted([f for f in os.listdir(generated_dir) if f.endswith(\".png\")])\n",
        "        self.seg_files = sorted([f for f in os.listdir(seg_dir) if f.endswith(\".png\")])\n",
        "        self.candidate_files = sorted([f for f in os.listdir(candidate_dir) if f.endswith(\".png\")])\n",
        "\n",
        "        self.candidate_map = {f: i for i, f in enumerate(self.candidate_files)}\n",
        "\n",
        "        assert len(self.generated_files) == len(self.seg_files), \\\n",
        "            \"Mismatch tra immagini generate e segmentate!\"\n",
        "        assert len(self.generated_files) <= len(self.ground_files), \\\n",
        "            \"PiÃ¹ immagini generate che ground!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.generated_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ground_filename = self.ground_files[idx]\n",
        "        image_id = os.path.splitext(ground_filename)[0]\n",
        "        gt_candidate_filename = f\"input{image_id}.png\"\n",
        "\n",
        "        if gt_candidate_filename not in self.candidate_map:\n",
        "            raise ValueError(f\"Candidato corretto {gt_candidate_filename} non trovato\")\n",
        "\n",
        "        ground_img = Image.open(os.path.join(self.ground_dir, ground_filename)).convert(\"RGB\")\n",
        "        gen_img = Image.open(os.path.join(self.generated_dir, self.generated_files[idx])).convert(\"RGB\")\n",
        "        seg_img = Image.open(os.path.join(self.seg_dir, self.seg_files[idx])).convert(\"RGB\")\n",
        "        gt_candidate_img = Image.open(os.path.join(self.candidate_dir, gt_candidate_filename)).convert(\"RGB\")\n",
        "\n",
        "        all_candidates = set(self.candidate_files)\n",
        "        all_candidates.remove(gt_candidate_filename)\n",
        "        negative_candidates = random.sample(list(all_candidates), self.num_candidates - 1)\n",
        "\n",
        "        candidate_imgs = []\n",
        "        gt_index = random.randint(0, self.num_candidates - 1)\n",
        "\n",
        "        for i in range(self.num_candidates):\n",
        "            if i == gt_index:\n",
        "                img = gt_candidate_img\n",
        "            else:\n",
        "                fname = negative_candidates.pop()\n",
        "                img = Image.open(os.path.join(self.candidate_dir, fname)).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            candidate_imgs.append(img)\n",
        "\n",
        "        if self.transform:\n",
        "            ground_img = self.transform(ground_img)\n",
        "            gen_img = self.transform(gen_img)\n",
        "            seg_img = self.transform(seg_img)\n",
        "\n",
        "        candidates_tensor = torch.stack(candidate_imgs, dim=0)\n",
        "\n",
        "        return {\n",
        "            \"ground\": ground_img,\n",
        "            \"generated\": gen_img,\n",
        "            \"seg\": seg_img,\n",
        "            \"candidates\": candidates_tensor,\n",
        "            \"gt_index\": gt_index\n",
        "        }\n"
      ],
      "metadata": {
        "id": "WQqpA5e62JrU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate_matching_topk(model, dataloader, device, topk=[1, 5]):\n",
        "    model.eval()\n",
        "    correct_at_k = {k: 0 for k in topk}\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            G = batch[\"ground\"].to(device)\n",
        "            A = batch[\"generated\"].to(device)\n",
        "            S = batch[\"seg\"].to(device)\n",
        "            C_all = batch[\"candidates\"].to(device)\n",
        "            gt_indices = batch[\"gt_index\"]\n",
        "\n",
        "            B, N, C, H, W = C_all.shape\n",
        "            C_all_flat = C_all.view(B * N, C, H, W)\n",
        "\n",
        "            embed_G, _ = model(G, A, S, C_all[:, 0])\n",
        "            embed_G = embed_G.unsqueeze(1)\n",
        "\n",
        "            embed_C = []\n",
        "            for i in range(N):\n",
        "                _, embed_ci = model(G, A, S, C_all[:, i])\n",
        "                embed_C.append(embed_ci)\n",
        "            embed_C = torch.stack(embed_C, dim=1)\n",
        "\n",
        "            sims = F.cosine_similarity(embed_G, embed_C, dim=2)\n",
        "\n",
        "            ranks = sims.argsort(dim=1, descending=True)\n",
        "\n",
        "            for k in topk:\n",
        "                for i in range(B):\n",
        "                    if gt_indices[i] in ranks[i, :k]:\n",
        "                        correct_at_k[k] += 1\n",
        "\n",
        "            total += B\n",
        "\n",
        "    for k in topk:\n",
        "        acc = correct_at_k[k] / total\n",
        "        print(f\"Top-{k} accuracy: {acc:.4f}\")\n",
        "\n",
        "    return [correct_at_k[k] / total for k in topk]\n"
      ],
      "metadata": {
        "id": "gnUw226r2P15"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = MatchingEvalDataset(\n",
        "    ground_dir=\"/content/drive/MyDrive/Dataset_Computer_Vision/Dataset_CVUSA/Dataset_CVUSA/streetview\",\n",
        "    generated_dir=\"/content/drive/MyDrive/generated_images\",\n",
        "    seg_dir=\"/content/drive/MyDrive/generated_seg\",\n",
        "    candidate_dir=\"/content/drive/MyDrive/Dataset_Computer_Vision/Dataset_CVUSA/Dataset_CVUSA/bingmap\",\n",
        "    transform=transform_rgb,\n",
        "    num_candidates=10\n",
        ")\n",
        "\n",
        "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cdweDJ_72X6_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1, top5 = evaluate_matching_topk(model, eval_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv746Qs-2Ug-",
        "outputId": "7c4d6051-5002-4f4c-a0ec-ef438a6f1db0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 accuracy: 0.0898\n",
            "Top-5 accuracy: 0.5038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5ttAeK92kiH"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}